


# Flash Attention Implementation in PyTorch

## Overview

This repository contains a personal and simplified implementation of the Flash Attention mechanism in PyTorch. Flash Attention is a novel attention mechanism that is designed to be more efficient and scalable than traditional attention mechanisms.

![flashatt](flash.png)

## Source 

The amazing medium blog by Aleksa GordiÄ‡:
https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad

This amazing repo (which I heavily inspired myself) : 
https://github.com/kyegomez/FlashAttention20/tree/main
