# An implementation from scratch of the Vision Transformer (ViT) from the paper _An image is worth 16*16 pixels_

## Positional Embeddings

Instead of using positional embeddings like mentionned in the paper, I decided to use learnable positional embeddings (see the reason below

## Different Positional Encodings
